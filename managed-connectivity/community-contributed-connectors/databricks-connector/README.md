# Databricks Connector

This custom connector extracts metadata from Databricks into [BigQuery universal catalog](https://cloud.google.com/dataplex/docs/catalog-overview).

Custom connectors are part of the [Managed Connectivity framework](https://cloud.google.com/dataplex/docs/managed-connectivity-overview) and are responsible for the export of metadata from external systems into correctly formatted import files. Import of metadata files generated by this connector and building pipelines is covered below in the section [Import metadata into universal catalog](#import-metadata-into-universal-catalog). See the documentation [Develop a custom connector for metadata import](https://cloud.google.com/dataplex/docs/develop-custom-connector) for more information about custom connectors for universal catalog.

This is not an officially supported Google product and is provided on an as-is basis, without warranty. This project is not eligible for the [Google Open Source Software Vulnerability Rewards
Program](https://bughunters.google.com/open-source-security).

### Target objects and schemas:

Metadata for the following database objects is extracted by the connector:
|Object|Metadata Extracted|
|---------|------------|
|Tables|Table name, column names, column data types, column NULL/NOT NULL|
|Views|View name, column names, column data types, column NULL/NOT NULL|

### Parameters

The Databricks connector takes the following parameters:
|Parameter|Description|Default|Required/Optional|
|---------|------------|---|-------------|
|target_project_id|Google Cloud Project ID. Used in the generated metadata and defines the scope metadata will be imported into||REQUIRED|
|target_location_id|Google Cloud Region ID, or 'global'. Used in the generated metadata and indicates the region Entries will be associated with||REQUIRED|
|target_entry_group_id|Entry Group ID which the Entries will be associated with||REQUIRED|
|server_hostname|Databricks server hostname to connect to||REQUIRED|
|http_path|Databricks compute resources URL||REQUIRED|
|access_token_secret|Google Secret Manager ID holding the personal access token for Databricks. Format: projects/{project-number}/secrets/{secret-name}||REQUIRED|
|local_output_only|Generate metadata import file in local directory only, do not push to Cloud Storage|False|OPTIONAL|
|output_bucket|Cloud Storage bucket where the output file will be stored. Required if **--local_output_only False**||REQUIRED|
|output_folder|Folder in Cloud Storage bucket where the output metadata import file will be stored. Required if **--local_output_only False**||
|min_expected_entries|Minimum number of entries expected in generated metadata import file. If less file is not uploaded to Cloud Storage|-1|OPTIONAL|

Note: **target_project_id**, **target_location_id** and **target_entry_group_id** are used as string values in the generated metadata import file only and do not need to match the project where the connector is being run. These three values define the job scope used when importing the metadata into the catalog, see [components of a metadata job](https://cloud.google.com/dataplex/docs/import-metadata#components) for details.

### Prepare your Databricks environment:

Best practice is to connect to the database with a dedicated user that has the minimum privileges required to extract metadata.

1.  Generate a [personal access token](https://docs.databricks.com/dev-tools/auth.html#pat) for the user.
2.  Add the personal access token to the Secret Manager in your Google Cloud project and note the ID (format is: projects/{project-number}/secrets/{secret-name})
3.  Get the [server hostname and HTTP path](https://docs.databricks.com/integrations/jdbc/index.html#get-server-hostname-port-and-http-path) for your Databricks cluster.

## Running the connector from the command line

The metadata connector can be run from the command line by executing the main.py script.

#### Prerequisites

The following tools and libraries are required to run the connector:

- Python 3.x. Installation on Linux:
  ```
  sudo apt update
  sudo apt install python3 python3-dev python3-venv python3-pip
  ```
- Python Virtual Environment. To create the environment and activate:
  ```shell
  python3 -m venv env
  source env/bin/activate
  ```
- Java Runtime Environment (JRE)

  ```shell
  sudo apt install default-jre
  ```

- The user that runs the connector must be authenticated with a Google Cloud identity in order to access the APIs for Secret Manager and Cloud Storage. You can use [Application Default Credentials](https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login) for the connector. If you are not running the connector in a Google Cloud managed environment then you need to install the [Google Cloud SDK](https://cloud.google.com/sdk/docs/install).

```bash
    gcloud auth application-default login
```

Note: The authenticated user must have the following IAM roles in the project where the connector runs:

- roles/secretmanager.secretAccessor
- roles/storage.objectUser

#### Set-up

- Ensure you are in the root directory of the connector

  ```bash
  cd databricks-connector
  ```

- Download the Databricks JDBC driver:

  ```bash
  wget https://repo1.maven.org/maven2/com/databricks/databricks-jdbc/2.6.36/databricks-jdbc-2.6.36.jar -O DatabricksJDBC42.jar
  ```

- Install python dependencies

  ```bash
  pip3 install -r requirements.txt
  ```

- The user that runs the connector must be authenticated with a Google Cloud identity in order to access the APIs for Secret Manager and Cloud Storage. You can use [Application Default Credentials](https://cloud.google.com/sdk/gcloud/reference/auth/application-default/login) to do this. If you are not running the connector in a Google Cloud managed environment then need to [install the Google Cloud SDK](https://cloud.google.com/sdk/docs/install).

Note the authenticated user have the following roles in the project:

- roles/secretmanager.secretAccessor
- roles/storage.objectUser

```bash
    gcloud auth application-default login
```

#### Run the connector

To execute metadata extraction run the following command, substituting appropriate values and parameters for your environment as needed:

```shell
python3 main.py \
--target_project_id gcp-project-id \
--target_location_id us-central1 \
--target_entry_group_id databricks \
--server_hostname "dbc-a1b2345c-d6e7.cloud.databricks.com" \
--http_path "sql/protocolv1/o/1234567890123456/1234-567890-abcdefgh" \
--access_token_secret projects/gcp-project-number/secrets/databricks-token \
--output_bucket my-gcs-bucket \
--output_folder databricks
```

### Connector Output:

The connector generates a metadata import file in JSONL format as described [in the documentation](https://cloud.google.com/dataplex/docs/import-metadata#metadata-import-file) and stores the file locally in the 'output' directory. The connector also uploads the file to the Google Cloud Storage bucket and folder specified in the **--output_bucket** and **--output_folder** parameters unless **--local-output_only True** is used.

A sample output from the Databricks connector can be found in the [sample](sample/) directory.

## Import metadata into universal catalog

Before importing metadata into universal catalog, the Entry Group and all Entry Types and Aspect Types found in the metadata file must exist in the target project and location (either a Google Cloud region, or 'global'). This connector requires the following Entry Group, Entry Types and Aspect Types:

| Catalog Object | IDs required by connector                                                                                              |
| -------------- | ---------------------------------------------------------------------------------------------------------------------- |
| Entry Group    | Defined in **--target_entry_group_id** when metadata is extracted                                                      |
| Entry Type     | **databricks-catalog**&nbsp;&nbsp;**databricks-schema**&nbsp;&nbsp;**databricks-table**&nbsp;&nbsp;**databricks-view** |
| Aspect Type    | **databricks-catalog**&nbsp;&nbsp;**databricks-schema**&nbsp;&nbsp;**databricks-table**&nbsp;&nbsp;**databricks-view** |

See [manage entries and create custom sources](https://cloud.google.com/dataplex/docs/ingest-custom-sources) for instructions on creating Entry Groups, Entry Types, and Aspect Types.

To manually import a metadata import file generated by this connector into universal catalog see [Import metadata using a custom pipeline](https://cloud.google.com/dataplex/docs/import-metadata#import-metadata) for instructions on calling the API and considerations when importing custom metadata.

To create an end-to-end pipeline which extracts metadata and starts an Import API job to bring the generated file into universal catalog, see the section [Create an end-to-end metadata extraction and import pipeline to universal catalog](#create-an-end-to-end-metadata-extraction-and-import-pipeline-to-universal-catalog)

## Build a container and run the connector with Dataproc Serverless

Building a Docker container for the connector allows it to be run from a variety of Google Cloud services including [Dataproc Serverless](https://cloud.google.com/dataproc-serverless/docs).

### Building the container (one-time task)

1.  Ensure docker is installed in your environment.
2.  Edit [build_and_push_docker.sh](build_and_push_docker.sh) and set the PROJECT_ID AND REGION_ID
3.  Ensure the user that runs the script is authenticated with a Google Cloud identify which has (Artifact Registry Writer)[https://cloud.google.com/artifact-registry/docs/access-control#roles] role on the Artfiact Registry in your project.
4.  Make the script executable and run:

    ```bash
    chmod a+x build_and_push_docker.sh
    ./build_and_push_docker.sh
    ```

    This will build a Docker container called **catalog-databricks-pyspark** and store it in Artifact Registry. This process can take take up to 5 minutes.

### Run a metadata extraction job with Dataproc Serverless

#### Set-up

Before you submit a job to Dataproc Serverless:

1.  Create or choose a Cloud Storage bucket to be used as a working directory for Dataproc. Use the path for the **--deps-bucket** parameter below.
2.  The service account given in **--service-account** below must have the IAM roles described [here](https://cloud.google.com/dataplex/docs/import-using-workflows-custom-source#required-roles) to successfully run Dataproc. You can use this [script](../common_scripts/grant_SA_dataproc_roles.sh) to grant the required roles to your service account.

Note:

- If **--service-account** is not provided then the default Compute Service Account for the project will be assumed.

#### Submit a Dataproc Serverless job

- Ensure you are in the root directory of the connector
  ```bash
  cd databricks-connector
  ```

Run the containerised metadata connector with the following command, substituting appropriate values for your environment and provding a unique batch ID in **--batch** :

```shell
gcloud dataproc batches submit pyspark \
    --project=gcp-project-id \
    --region=us-central1 \
    --batch=databricks-metadata-001 \
    --deps-bucket=dataplex-metadata-collection-bucket \
    --container-image=us-central1-docker.pkg.dev/gcp-project-id/docker-repo/catalog-databricks-pyspark:latest \
    --service-account=gcp-project-number-compute@developer.gserviceaccount.com \
    --jars=DatabricksJDBC42.jar \
    --version=1.2 \
    --network=default \
    main.py \
    --target_project_id my-gcp-project \
    --target_location_id us-central1 \
    --target_entry_group_id databricks \
    --server_hostname "dbc-a1b2345c-d6e7.cloud.databricks.com" \
    --http_path "sql/protocolv1/o/1234567890123456/1234-567890-abcdefgh" \
    --access_token_secret projects/gcp-project-number/secrets/databricks-token \
    --output_bucket my-gcs-bucket \
    --output_folder databricks_metadata
```

See the [documentation](https://cloud.google.com/sdk/gcloud/reference/dataproc/batches/submit/pyspark) for more information about Dataproc Serverless pyspark jobs.

## Create an end-to-end metadata extraction and import pipeline to universal catalog

An end-to-end metadata extraction and import pipeline with monitoring can be created using [Workflows](https://cloud.google.com/workflows) and scheduled to run on a regular basis.

Follow the documentation here: [Import metadata from a custom source using Workflows](https://cloud.google.com/dataplex/docs/import-using-workflows-custom-source) and use [this yaml file](https://github.com/GoogleCloudPlatform/cloud-dataplex/blob/main/managed-connectivity/cloud-workflows/byo-connector/templates/byo-connector.yaml) as a template.
